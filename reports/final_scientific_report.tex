\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{apacite}
\usepackage[margin=1.5cm]{geometry}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=blue,
    breaklinks=true
}
\usepackage[font=small,labelfont=bf,skip=1pt,position=bottom]{caption}
\captionsetup{format=plain,justification=justified,singlelinecheck=true}
\setlength{\columnsep}{0.5cm}
\setlength{\parskip}{0pt}
\setlength{\parindent}{0pt}
\setlength{\textfloatsep}{2pt plus 1pt minus 1pt}
\setlength{\floatsep}{2pt plus 1pt minus 1pt}
\setlength{\intextsep}{2pt plus 1pt minus 1pt}
\setlength{\abovecaptionskip}{1pt}
\setlength{\belowcaptionskip}{0pt}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowdisplayskip}{2pt}
\setlength{\abovedisplayskip}{2pt}

% Reduce section spacing
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{4pt}{2pt}
\titlespacing*{\subsection}{0pt}{3pt}{1pt}

\renewcommand{\refname}{} % Remove the automatic bibliography heading

\title{\textbf{Fairness-Analyse eines Stacking-Ensemble-Modells für Gesundheitsvorhersage}}
\author{Ola Hamza, Abdul Jamil Safi}
\date{Dezember 2025}

\begin{document}

\maketitle
\vspace{-12pt}

\begin{abstract}
\small
Diese Arbeit untersucht die Fairness eines Stacking-Ensemble-Modells (RF, SVM, MLP, HGB) zur Gesundheitsvorhersage bei Studierenden. Das Modell erreicht F1=0.83 (Std 0.083) und zeigt akzeptable Fairness über sechs Subgruppen (Gender × Schlaf) bei relaxierten Kriterien (Std F1 $<$ 0.10 für $n \geq 6$).
\end{abstract}
\vspace{-3pt}

\section{Einleitung}
Machine Learning in der Gesundheitsvorhersage erfordert Fairness über demografische Gruppen \cite{obermeyer2019}. Studien zeigen systematische Verzerrungen in KI-Systemen, die benachteiligte Gruppen diskriminieren. Diese Arbeit untersucht, ob ein Ensemble-Modell gleichwertige Performance über Subgruppen erzielt.

\textbf{Stand der Forschung:} Stacking-Ensembles kombinieren diverse Modellstärken zur Verbesserung der Generalisierung \cite{wolpert1992}. Géron \cite{geron2022} empfiehlt Stacking mit Logistic Regression als Meta-Modell für Klassifikationsaufgaben. SMOTE adressiert Klassenungleichgewichte durch synthetische Beispielgenerierung \cite{chawla2002}. Fairness wird durch F1-Score-Standardabweichung über Subgruppen quantifiziert \cite{mehrabi2021}.

\subsection{Forschungsfrage}
\textbf{Hat das Klassifikationsmodell, basierend auf dem Stacking-Ensemble-Ansatz und fortgeschrittenen Time-Series-Features, gleichwertige Vorhersageleistung für den Overall Health Score über verschiedene Subgruppen (Gender × Schlafkategorie)?}

Diese zentrale Forschungsfrage adressiert die Fairness des Modells über demografische und Verhaltens-Subgruppen hinweg. Die Beantwortung erfolgt durch quantitative Evaluation mit strikten (Std F1 $<$ 0.05) und relaxierten (Std F1 $<$ 0.10 für $n\geq 6$) Fairness-Kriterien.

\section{Methodik}

\subsection{Datensatz und Features}
\textbf{Datensatz:} 100 chronologisch sortierte Studierenden-Beobachtungen (5-Minuten-Intervalle) aus dem Biosensor Student Health \& Fitness Dataset \cite{ziya2024} mit demografischen (Alter, Gender), physiologischen (Herzfrequenz, Blutdruck, Körpertemperatur, Blutsauerstoff) und verhaltensbezogenen Features (Aktivität, Schlaf, Stress, Hydration). \textbf{Zielvariable:} Overall Health Score binär (>80 = High, $\leq$80 = Low).

\textbf{Feature Engineering:} Rolling Statistics über 30-Minuten-Fenster: Mean und Std für Herzfrequenz und Aktivitätslevel. Schlafkategorisierung: Short (<6h), Normal (6-8h), Long (>8h). Subgruppen-Feature: Gender\_Sleep\_Group (z.B., "Male\_Normal").

\subsection{Modellarchitektur}
\textbf{Stacking-Ensemble} mit 4 kalibrierten Basismodellen, optimiert via Bayesian Optimization:
\begin{itemize}
\item \textbf{Random Forest:} Bayesian Search über n\_estimators (100-300), max\_depth (5-15), min\_samples\_leaf (1-4), mit CalibratedClassifierCV (isotonic)
\item \textbf{SVM:} Bayesian Search über C (0.1-10.0), gamma (0.001-1.0), kernel (rbf/poly), mit CalibratedClassifierCV (sigmoid)
\item \textbf{MLP:} Bayesian Search über hidden\_layers, activation (relu/tanh), alpha, learning\_rate, mit CalibratedClassifierCV (sigmoid)
\item \textbf{HGB:} Bayesian Search über learning\_rate (0.01-0.3), max\_depth (3-10), max\_iter (50-150), mit CalibratedClassifierCV (isotonic)
\end{itemize}

\textbf{Meta-Modell:} Logistic Regression mit inverse Subgruppen-Gewichten, C=1.0, class\_weight='balanced'.

\textbf{Hyperparameter-Optimierung:} Bayesian Optimization (BayesSearchCV, scikit-optimize) wurde für alle Level-0-Modelle eingesetzt. Diese Methode ist effizienter als Grid/Random Search, da sie eine probabilistische Surrogatfunktion (Gaussian Process) nutzt, um vielversprechende Hyperparameter-Regionen gezielt zu explorieren. Jedes Modell durchlief 15-20 Iterationen mit 3-facher Kreuzvalidierung zur F1-Score-Maximierung.

\textbf{Fairness-Techniken:} SMOTE-Balancierung, Wahrscheinlichkeitskalibrierung, per-subgroup Threshold-Tuning (0.1-0.9, Schritt 0.02), Sample-Gewichtung (inverse Gruppengröße).

\subsection{Evaluation}
\textbf{Chronological Split:} Subgruppenbasierte Aufteilung (60/10/30) verhindert temporale Datenleckage. \textbf{Metriken:} F1, Accuracy, ROC-AUC. \textbf{Fairness-Kriterien:} Strikt (Std F1 $<$ 0.05 alle Gruppen), Realistisch (Std F1 $<$ 0.10 für $n \geq 6$).

\textbf{Dashboard-Implementierung:} Ein interaktives Streamlit-Dashboard ermöglicht die Visualisierung der Subgruppen-Performance, Kalibrierungskurven und Vorhersagen auf neuen Daten. Das Dashboard bietet zwei Hauptfunktionen: (1) Model Analysis mit Live-Visualisierungen (F1 by Subgroup, Sample Size vs F1, Accuracy Comparison), und (2) Predict New Data mit CSV-Upload und automatischer Feature-Validierung.

\subsection{Teamarbeit und Aufgabenteilung}
Die Projektdurchführung erfolgte in enger Zusammenarbeit zwischen den Autoren. \textbf{Abdul Jamil Safi} übernahm die technische Implementierung: Python-Code-Entwicklung (health\_prediction\_pipeline.py), SMOTE-Integration, Kalibrierungstechniken, Streamlit-Dashboard-Erstellung, GitHub-Repository-Management und Datenanalyse. \textbf{Ola Hamza} verantwortete die wissenschaftliche Dokumentation: LaTeX-Report-Erstellung, theoretische Grundlagen, Literaturrecherche, Methodik-Beschreibung und Ergebnisinterpretation. Gemeinsame Aufgaben umfassten Strategieentwicklung, Code-Review, Fairness-Kriterien-Definition und Diskussion der Resultate.

\section{Resultate}

\textbf{Gesamtperformance:} Das Stacking-Ensemble erreichte auf dem Test-Set folgende Metriken: \textbf{Accuracy=0.65}, \textbf{F1=0.78}, \textbf{ROC-AUC=0.48}. Die moderate Accuracy deutet auf Herausforderungen bei der Klassifikation hin, während der solide F1-Score (0.78) auf ausgewogene Precision-Recall-Verhältnisse hindeutet. Die niedrige ROC-AUC (0.48) zeigt Schwierigkeiten bei der Wahrscheinlichkeitskalibrierung, trotz Einsatz von CalibratedClassifierCV.

\subsection{Subgruppen-Analyse}

Tabelle \ref{tab:performance} zeigt die detaillierte Performance über alle sechs Subgruppen. Female\_Normal erreicht die beste Performance (F1=0.91, Acc=0.83) mit optimaler Threshold=0.5 und ROC-AUC=0.8. Male\_Short zeigt die schwächste Performance (F1=0.67, Acc=0.50) bei gleicher Threshold=0.5 und ROC-AUC=0.5. Die Standardabweichung des F1-Scores beträgt 0.092 über alle Gruppen, wobei Gruppen mit $n \geq 6$ eine Std von 0.091 aufweisen.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{figures/performance_table.png}
\caption{Detaillierte Performance-Metriken nach Subgruppen (Gender × Schlafkategorie). Die Tabelle zeigt Stichprobengröße, optimale Threshold-Werte, Accuracy, F1-Score, ROC-AUC und Anteil positiver Klasse für jede Subgruppe.}
\label{tab:performance}
\end{figure}

\textbf{Statistische Zusammenfassung:} Mean F1=0.81 (alle Gruppen), Best=Female\_Normal (0.91), Worst=Male\_Short (0.67). Die True High Health Prozentsätze variieren zwischen 50\% (Male\_Short) und 100\% (Other\_Short), was unterschiedliche Klassenverteilungen in den Subgruppen reflektiert.

\textbf{Fairness-Analyse:} Std F1=0.092 (alle), 0.091 ($n\geq 6$) erfüllt relaxierte Kriterien. Größere Gruppen zeigen stabilere Performance (Abbildung \ref{fig:f1_subgroup}).

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{figures/f1_by_subgroup.png}
\caption{F1-Score-Verteilung nach Subgruppen. Farbcodierung: Grün (F1 $>$ 0.85), Gelb (0.75-0.85), Rot ($<$ 0.75). Female\_Normal zeigt höchste Performance (0.91), Male\_Short niedrigste (0.67).}
\label{fig:f1_subgroup}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{figures/rf_subgroup_analysis.png}
\caption{Vergleichende Subgruppenanalyse des Random Forest Modells. Links: Accuracy-Werte nach Gender und Schlafkategorie. Rechts: Entsprechende F1-Scores. Normal-Sleeper zeigen konsistent höhere Performance als Short-Sleeper.}
\label{fig:rf_subgroup}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{figures/size_vs_f1.png}
\caption{Zusammenhang zwischen Stichprobengröße und F1-Score. Die gestrichelte Linie zeigt den linearen Trend. Größere Subgruppen ($n \geq 6$) erreichen stabilere Performance (F1 > 0.80), während kleinere Gruppen höhere Varianz aufweisen.}
\label{fig:size_f1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{figures/f1_vs_accuracy.png}
\caption{Vergleich von F1-Score und Accuracy über alle Subgruppen. Punkte repräsentieren einzelne Subgruppen. Die diagonale Referenzlinie zeigt perfekte Übereinstimmung. Abweichungen indizieren unterschiedliche Klassenverteilungen.}
\label{fig:f1_acc}
\end{figure}

\section{Diskussion}

Das Modell erfüllt relaxierte Fairness-Kriterien (Std<0.10) für größere Gruppen ($n \geq 6$), jedoch nicht strenge Kriterien (Std<0.05). Abbildung \ref{fig:f1_subgroup} zeigt deutliche Performanzunterschiede: Female\_Normal erreicht F1=0.91 (grün), während Male\_Short nur F1=0.67 (rot) erzielt.

Abbildung \ref{fig:size_f1} bestätigt die positive Korrelation zwischen Stichprobengröße und F1-Score – größere Gruppen (n=6-7) liegen bei F1>0.80, kleinere (n=4) zeigen höhere Varianz. Dies unterstreicht die Bedeutung ausreichender Trainingsdaten für faire Vorhersagen.

Abbildung \ref{fig:f1_acc} verdeutlicht das Trade-off zwischen Metriken: Hohe F1-Scores korrelieren nicht immer mit hoher Accuracy, was auf unterschiedliche Klassenverteilungen in Subgruppen hindeutet.

Die Random Forest Subgruppenanalyse (Abbildung \ref{fig:rf_subgroup}) zeigt konsistente Muster über Geschlechter und Schlafkategorien. Beide Metriken (Accuracy und F1-Score) variieren zwischen Subgruppen, wobei die Normal-Schlafkategorie tendenziell bessere Performance zeigt als Short-Sleeper.

\textbf{Fairness-Strategien:} SMOTE generierte synthetische Minderheitsklassen-Beispiele, was die Klassenbalance verbesserte. CalibratedClassifierCV mit isotonic (RF, HGB) und sigmoid (SVM, MLP) Regression lieferte reliable Wahrscheinlichkeitsschätzungen. Per-subgroup Threshold-Tuning optimierte F1-Scores individuell (0.40 für Male\_Normal, 0.50 für andere), jedoch bleibt ROC-AUC=0.48 suboptimal.

\textbf{Interpretation:} Die Ergebnisse zeigen, dass Fairness stark von Datenverfügbarkeit abhängt. Subgruppen mit $n\geq 6$ erreichen Std F1=0.091, während kleinere Gruppen durch Overfitting und unzureichende Repräsentation leiden. Das Modell erfüllt somit realistische, aber nicht ideale Fairness-Anforderungen.

\textbf{Limitationen:} Kleiner Datensatz (n=100) begrenzt statistische Power. Fehlende Long-Sleep-Kategorie in Test-Daten aufgrund der chronologischen Aufteilung.

\textbf{Praktische Implikationen:} Das Modell eignet sich für explorative Analysen, jedoch nicht für klinische Entscheidungen ohne weitere Validierung. Größere Datenmengen (n>20 pro Subgruppe) würden Fairness signifikant verbessern.

\section{Zusammenfassung und Ausblick}

Das Stacking-Ensemble zeigt vernünftige Fairness über demografische Subgruppen bei relaxierten Kriterien (Std F1<0.10 für $n\geq 6$). Die Kombination aus SMOTE, Kalibrierung und per-subgroup Thresholds adressiert Fairness-Herausforderungen teilweise erfolgreich. Größere Gruppen profitieren von stabilerer Performance, während kleinere Gruppen höhere Varianz aufweisen.

\textbf{Zukünftige Arbeiten:} (1) Erweiterte temporale Features: Multi-Step Lag-Features, LSTM/GRU für Sequenzmodellierung. (2) Erweiterte Kalibrierungstechniken: Venn-ABERS Predictors, Temperature Scaling. (3) Größere Datensätze: Mindestens n=50 pro Subgruppe für robuste Evaluation. (4) Externe Validierung: Test auf unabhängigen Kohorten zur Generalisierungsprüfung. (5) Fairness-Constraints: Integration von Fairness-Metriken direkt in die Optimierungsfunktion.

\textbf{Code-Verfügbarkeit:} Der vollständige Quellcode, das trainierte Modell, die Daten und das interaktive Dashboard sind öffentlich verfügbar unter: \url{https://github.com/safiabduljamil/ml-health-equality}

\section{Literatur}
\vspace{-8pt}

\bibliographystyle{apacite}
\bibliography{references}
\end{document}
